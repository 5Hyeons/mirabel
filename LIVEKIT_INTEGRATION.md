# LiveKit + WebGL í†µí•© ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” **Unity WebGL** ê¸°ë°˜ 3D ì•„ë°”íƒ€ì™€ **LiveKit Agents**ë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ AI ìƒë‹´ ì‹œìŠ¤í…œì˜ í†µí•© ë°©ë²•ì„ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ğŸ— ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      React Web Application                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  UI Components      â”‚           â”‚  Unity WebGL           â”‚    â”‚
â”‚  â”‚  (ìƒíƒœ, ë²„íŠ¼ ë“±)      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (3D Avatar)           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                 â”‚                â”‚
â”‚                                                 â”‚ jslib Bridge   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                           WebSocket (LiveKit)    â”‚
                                                  â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚      LiveKit Server (SFU)          â”‚
                        â”‚  - Audio Routing                   â”‚
                        â”‚  - RPC Handling                    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â”‚ Audio Stream + RPC
                                     â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚    Python Agent                â”‚
                        â”‚  - STT (Whisper/Google)        â”‚
                        â”‚  - LLM (GPT-4/Claude)          â”‚
                        â”‚  - TTS (OpenAI/ElevenLabs)     â”‚
                        â”‚  - Face Animation (TalkMotion) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ® Unity WebGL í†µí•©

### 1. Unity í”„ë¡œì íŠ¸ ì„¤ì •

#### 1.1 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
unity-webgl/
â”œâ”€â”€ Assets/
â”‚   â”œâ”€â”€ LiveKit/                    # LiveKit Unity SDK
â”‚   â”œâ”€â”€ Plugins/
â”‚   â”‚   â””â”€â”€ WebGL/
â”‚   â”‚       â””â”€â”€ BridgeToReact.jslib # React â†” Unity í†µì‹ 
â”‚   â”œâ”€â”€ Scenes/
â”‚   â”‚   â””â”€â”€ AIConsultation.unity    # ìƒë‹´ ì”¬
â”‚   â”œâ”€â”€ Scripts/
â”‚   â”‚   â”œâ”€â”€ LiveKitManager.cs       # LiveKit ì—°ê²° ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ AvatarController.cs     # ì•„ë°”íƒ€ ì œì–´
â”‚   â”‚   â”œâ”€â”€ LipSyncController.cs    # ë¦½ì‹±í¬ ì œì–´
â”‚   â”‚   â””â”€â”€ ReactBridge.cs          # React ë¸Œë¦¿ì§€
â”‚   â””â”€â”€ Models/
â”‚       â””â”€â”€ DoctorAvatar.vrm        # VRM ì•„ë°”íƒ€
â”œâ”€â”€ Packages/
â”‚   â””â”€â”€ manifest.json
â””â”€â”€ ProjectSettings/
    â””â”€â”€ ProjectSettings.asset
```

---

#### 1.2 Build Settings

**File â†’ Build Settings â†’ WebGL**

```
Platform: WebGL
Compression Format: Gzip
Code Optimization: Master
Enable Exceptions: None
Strip Engine Code: Yes
Managed Stripping Level: High

Player Settings:
- Resolution:
  - Default Canvas Width: 800
  - Default Canvas Height: 600
  - Run In Background: Yes

- Publishing Settings:
  - Compression Format: Gzip
  - Data caching: Yes
  - WebGL Memory Size: 512 MB (ì¡°ì • ê°€ëŠ¥)

- Other Settings:
  - Api Compatibility Level: .NET Standard 2.1
  - Managed Stripping Level: High
```

---

### 2. LiveKit Unity SDK í†µí•©

#### 2.1 Package ì„¤ì¹˜

```json
// Packages/manifest.json
{
  "dependencies": {
    "com.livekit.livekit-unity": "https://github.com/livekit/client-sdk-unity.git#v0.3.0",
    "com.vrmc.univrm": "0.108.0",
    "com.unity.nuget.newtonsoft-json": "3.2.1"
  }
}
```

---

#### 2.2 LiveKitManager.cs

```csharp
using UnityEngine;
using LiveKit;
using System;
using System.Threading.Tasks;

public class LiveKitManager : MonoBehaviour
{
    [SerializeField] private string livekitUrl;
    [SerializeField] private AvatarController avatarController;

    private Room room;
    private bool isConnected = false;

    // Reactë¡œë¶€í„° í˜¸ì¶œë¨
    public async void ConnectToRoom(string token)
    {
        try
        {
            Debug.Log($"Connecting to LiveKit: {livekitUrl}");

            room = new Room();

            // Room ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ
            room.Connected += OnRoomConnected;
            room.Disconnected += OnRoomDisconnected;
            room.TrackSubscribed += OnTrackSubscribed;
            room.DataReceived += OnDataReceived;

            await room.Connect(livekitUrl, token);
        }
        catch (Exception ex)
        {
            Debug.LogError($"LiveKit connection error: {ex.Message}");
            SendToReact("onConnectionError", ex.Message);
        }
    }

    private void OnRoomConnected(Room room)
    {
        Debug.Log("Connected to LiveKit room");
        isConnected = true;
        SendToReact("onConnected", "success");
    }

    private void OnRoomDisconnected(Room room)
    {
        Debug.Log("Disconnected from LiveKit room");
        isConnected = false;
        SendToReact("onDisconnected", "");
    }

    private void OnTrackSubscribed(IRemoteTrack track, RemoteTrackPublication publication, RemoteParticipant participant)
    {
        Debug.Log($"Track subscribed: {track.Kind} from {participant.Identity}");

        if (track.Kind == TrackKind.Audio && participant.Identity.StartsWith("agent"))
        {
            // Agentì˜ ì˜¤ë””ì˜¤ íŠ¸ë™
            var audioTrack = track as RemoteAudioTrack;
            if (audioTrack != null)
            {
                // ì˜¤ë””ì˜¤ ì¬ìƒ (Unity AudioSourceì— ì—°ê²°)
                PlayAudioTrack(audioTrack);
            }
        }
    }

    private void OnDataReceived(byte[] data, RemoteParticipant participant)
    {
        // Agentë¡œë¶€í„° ë°ì´í„° ìˆ˜ì‹  (ë¦½ì‹±í¬ ì •ë³´ ë“±)
        string jsonData = System.Text.Encoding.UTF8.GetString(data);
        Debug.Log($"Data received: {jsonData}");

        var message = JsonUtility.FromJson<AgentMessage>(jsonData);

        if (message.type == "lipSync")
        {
            // ë¦½ì‹±í¬ ë°ì´í„° ì ìš©
            avatarController.ApplyLipSync(message.blendShapes);
        }
    }

    // Agentì—ê²Œ RPC í˜¸ì¶œ (ë‹µë³€ ì¤‘ë‹¨)
    public async void SendInterrupt()
    {
        if (!isConnected) return;

        try
        {
            var response = await room.LocalParticipant.PerformRpc(
                destinationIdentity: "agent",
                method: "interrupt",
                payload: ""
            );

            Debug.Log("Interrupt signal sent to agent");
            SendToReact("onInterruptSent", "success");
        }
        catch (Exception ex)
        {
            Debug.LogError($"Failed to send interrupt: {ex.Message}");
        }
    }

    private void PlayAudioTrack(RemoteAudioTrack audioTrack)
    {
        // Unity AudioSourceì— ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì—°ê²°
        AudioSource audioSource = gameObject.AddComponent<AudioSource>();
        audioSource.clip = AudioClip.Create(
            "AgentAudio",
            audioTrack.SampleRate * 10, // 10ì´ˆ ë²„í¼
            audioTrack.Channels,
            audioTrack.SampleRate,
            true
        );
        audioSource.Play();

        // ì˜¤ë””ì˜¤ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
        audioTrack.AudioFrameReceived += (frame) =>
        {
            // ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ AudioSourceì— ì „ë‹¬
            // ë¦½ì‹±í¬ ì»¨íŠ¸ë¡¤ëŸ¬ì—ë„ ì „ë‹¬
            if (avatarController != null)
            {
                avatarController.ProcessAudioFrame(frame.Data);
            }
        };
    }

    // Reactë¡œ ë©”ì‹œì§€ ì „ì†¡
    private void SendToReact(string eventName, string data)
    {
        #if UNITY_WEBGL && !UNITY_EDITOR
        ReactBridge.SendMessage(eventName, data);
        #endif
    }

    public void Disconnect()
    {
        if (room != null)
        {
            room.Disconnect();
            room = null;
        }
    }

    private void OnDestroy()
    {
        Disconnect();
    }
}

[Serializable]
public class AgentMessage
{
    public string type;
    public float[] blendShapes;
}
```

---

### 3. ì•„ë°”íƒ€ ì œì–´ (VRM + Lip Sync)

#### 3.1 AvatarController.cs

```csharp
using UnityEngine;
using UniVRM10;

public class AvatarController : MonoBehaviour
{
    [SerializeField] private Vrm10Instance vrmInstance;
    private LipSyncController lipSyncController;

    private void Start()
    {
        lipSyncController = GetComponent<LipSyncController>();

        if (vrmInstance == null)
        {
            Debug.LogError("VRM Instance not assigned!");
        }
    }

    // ë¦½ì‹±í¬ ë¸”ë Œë“œì…°ì´í”„ ì ìš© (Agentë¡œë¶€í„° ìˆ˜ì‹ )
    public void ApplyLipSync(float[] blendShapes)
    {
        if (vrmInstance == null || blendShapes == null) return;

        // blendShapes ë°°ì—´: [A, I, U, E, O, Blink]
        var blendShapeProxy = vrmInstance.Runtime.Expression;

        blendShapeProxy.SetWeight(ExpressionKey.Aa, blendShapes[0]);   // A
        blendShapeProxy.SetWeight(ExpressionKey.Ih, blendShapes[1]);   // I
        blendShapeProxy.SetWeight(ExpressionKey.Ou, blendShapes[2]);   // U
        blendShapeProxy.SetWeight(ExpressionKey.Ee, blendShapes[3]);   // E
        blendShapeProxy.SetWeight(ExpressionKey.Oh, blendShapes[4]);   // O
        blendShapeProxy.SetWeight(ExpressionKey.Blink, blendShapes[5]); // Blink
    }

    // ì˜¤ë””ì˜¤ í”„ë ˆì„ìœ¼ë¡œ ë¦½ì‹±í¬ ìƒì„± (ë¡œì»¬ ì²˜ë¦¬)
    public void ProcessAudioFrame(float[] audioData)
    {
        if (lipSyncController != null)
        {
            lipSyncController.ProcessAudio(audioData);
        }
    }

    // ì•„ë°”íƒ€ ì• ë‹ˆë©”ì´ì…˜ (Idle, Speaking ë“±)
    public void SetAnimationState(string state)
    {
        // Animator íŒŒë¼ë¯¸í„° ì„¤ì •
        var animator = GetComponent<Animator>();
        if (animator != null)
        {
            animator.SetTrigger(state);
        }
    }
}
```

---

#### 3.2 LipSyncController.cs

```csharp
using UnityEngine;

public class LipSyncController : MonoBehaviour
{
    [SerializeField] private float sensitivity = 1.5f;
    [SerializeField] private float smoothing = 10f;

    private float[] currentBlendShapes = new float[6]; // A, I, U, E, O, Blink
    private float[] targetBlendShapes = new float[6];

    // ì˜¤ë””ì˜¤ ë°ì´í„°ë¡œë¶€í„° ë¦½ì‹±í¬ ê³„ì‚°
    public void ProcessAudio(float[] audioData)
    {
        // ê°„ë‹¨í•œ ë³¼ë¥¨ ê¸°ë°˜ ë¦½ì‹±í¬ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ í•„ìš”)
        float volume = CalculateVolume(audioData);

        // ë³¼ë¥¨ì— ë”°ë¼ ì… ëª¨ì–‘ ì„¤ì •
        if (volume > 0.1f)
        {
            // ë§í•˜ëŠ” ì¤‘: Aì™€ I ë¸”ë Œë“œì…°ì´í”„ ì‚¬ìš©
            targetBlendShapes[0] = Mathf.Clamp01(volume * sensitivity * 0.8f); // A
            targetBlendShapes[1] = Mathf.Clamp01(volume * sensitivity * 0.5f); // I
        }
        else
        {
            // ì¹¨ë¬µ: ì… ë‹«ê¸°
            targetBlendShapes[0] = 0f;
            targetBlendShapes[1] = 0f;
        }

        // ìì—°ìŠ¤ëŸ¬ìš´ ê¹œë°•ì„
        if (Random.value < 0.01f) // 1% í™•ë¥ ë¡œ ê¹œë°•ì„
        {
            targetBlendShapes[5] = 1f; // Blink
        }
        else
        {
            targetBlendShapes[5] = 0f;
        }
    }

    private void Update()
    {
        // ë¶€ë“œëŸ¬ìš´ ì „í™˜ (Lerp)
        for (int i = 0; i < currentBlendShapes.Length; i++)
        {
            currentBlendShapes[i] = Mathf.Lerp(
                currentBlendShapes[i],
                targetBlendShapes[i],
                smoothing * Time.deltaTime
            );
        }

        // AvatarControllerì— ì ìš©
        GetComponent<AvatarController>().ApplyLipSync(currentBlendShapes);
    }

    private float CalculateVolume(float[] audioData)
    {
        float sum = 0f;
        for (int i = 0; i < audioData.Length; i++)
        {
            sum += Mathf.Abs(audioData[i]);
        }
        return sum / audioData.Length;
    }
}
```

---

### 4. React â†” Unity í†µì‹ 

#### 4.1 BridgeToReact.jslib

```javascript
// Assets/Plugins/WebGL/BridgeToReact.jslib

var ReactBridge = {
    $bridgeState: {
        reactCallbacks: {}
    },

    // Reactì— ë©”ì‹œì§€ ì „ì†¡
    SendMessage: function(eventName, data) {
        var event = UTF8ToString(eventName);
        var payload = UTF8ToString(data);

        console.log("[Unity â†’ React]", event, payload);

        // Reactì˜ ì»¤ìŠ¤í…€ ì´ë²¤íŠ¸ ë°œìƒ
        window.dispatchEvent(new CustomEvent('UnityMessage', {
            detail: {
                event: event,
                data: payload
            }
        }));
    },

    // Reactë¡œë¶€í„° í˜¸ì¶œ ë“±ë¡
    RegisterCallback: function(eventName, callback) {
        var event = UTF8ToString(eventName);
        bridgeState.reactCallbacks[event] = callback;
        console.log("[React â†’ Unity] Callback registered:", event);
    }
};

autoAddDeps(ReactBridge, '$bridgeState');
mergeInto(LibraryManager.library, ReactBridge);
```

---

#### 4.2 ReactBridge.cs

```csharp
using System.Runtime.InteropServices;
using UnityEngine;

public class ReactBridge : MonoBehaviour
{
    [DllImport("__Internal")]
    private static extern void SendMessage(string eventName, string data);

    // Unity â†’ React
    public static void SendToReact(string eventName, string data)
    {
        #if UNITY_WEBGL && !UNITY_EDITOR
        SendMessage(eventName, data);
        #else
        Debug.Log($"[Mock] Unity â†’ React: {eventName}, {data}");
        #endif
    }

    // React â†’ Unity (Unity í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ)
    public void OnReactMessage(string message)
    {
        Debug.Log($"[React â†’ Unity] Message received: {message}");

        // JSON íŒŒì‹± í›„ ì²˜ë¦¬
        var data = JsonUtility.FromJson<ReactMessage>(message);

        switch (data.action)
        {
            case "connect":
                FindObjectOfType<LiveKitManager>().ConnectToRoom(data.token);
                break;

            case "interrupt":
                FindObjectOfType<LiveKitManager>().SendInterrupt();
                break;

            case "disconnect":
                FindObjectOfType<LiveKitManager>().Disconnect();
                break;
        }
    }
}

[System.Serializable]
public class ReactMessage
{
    public string action;
    public string token;
}
```

---

### 5. React í†µí•©

#### 5.1 UnityWebGL ì»´í¬ë„ŒíŠ¸

```typescript
// components/UnityWebGL.tsx
import React, { useEffect, useRef } from 'react';
import { Unity, useUnityContext } from 'react-unity-webgl';

interface UnityWebGLProps {
  livekitToken: string;
  onConnected: () => void;
  onDisconnected: () => void;
  onError: (error: string) => void;
}

export default function UnityWebGL({ livekitToken, onConnected, onDisconnected, onError }: UnityWebGLProps) {
  const { unityProvider, sendMessage, addEventListener, removeEventListener, isLoaded } = useUnityContext({
    loaderUrl: '/unity/Build.loader.js',
    dataUrl: '/unity/Build.data',
    frameworkUrl: '/unity/Build.framework.js',
    codeUrl: '/unity/Build.wasm'
  });

  // Unityë¡œë¶€í„° ì´ë²¤íŠ¸ ìˆ˜ì‹ 
  useEffect(() => {
    addEventListener('UnityMessage', handleUnityMessage);

    return () => {
      removeEventListener('UnityMessage', handleUnityMessage);
    };
  }, [addEventListener, removeEventListener]);

  function handleUnityMessage(event: CustomEvent) {
    const { event: eventName, data } = event.detail;

    console.log('[React â† Unity]', eventName, data);

    switch (eventName) {
      case 'onConnected':
        onConnected();
        break;
      case 'onDisconnected':
        onDisconnected();
        break;
      case 'onConnectionError':
        onError(data);
        break;
    }
  }

  // Unity ë¡œë“œ ì™„ë£Œ ì‹œ LiveKit ì—°ê²°
  useEffect(() => {
    if (isLoaded && livekitToken) {
      connectToLiveKit();
    }
  }, [isLoaded, livekitToken]);

  function connectToLiveKit() {
    sendMessage('LiveKitManager', 'ConnectToRoom', livekitToken);
  }

  function handleInterrupt() {
    sendMessage('LiveKitManager', 'SendInterrupt', '');
  }

  function handleDisconnect() {
    sendMessage('LiveKitManager', 'Disconnect', '');
  }

  return (
    <div className="unity-container">
      <Unity
        unityProvider={unityProvider}
        style={{ width: '100%', height: '100%' }}
      />

      {/* ë””ë²„ê·¸ ì»¨íŠ¸ë¡¤ */}
      {process.env.NODE_ENV === 'development' && (
        <div className="debug-controls">
          <button onClick={connectToLiveKit}>Connect</button>
          <button onClick={handleInterrupt}>Interrupt</button>
          <button onClick={handleDisconnect}>Disconnect</button>
        </div>
      )}
    </div>
  );
}
```

---

## ğŸ Python Agent êµ¬í˜„

### 1. Agent í”„ë¡œì íŠ¸ êµ¬ì¡°

```
agent/
â”œâ”€â”€ main.py                 # ë©”ì¸ ì§„ì…ì 
â”œâ”€â”€ agent.py                # Agent ë¡œì§
â”œâ”€â”€ requirements.txt
â””â”€â”€ config.py               # ì„¤ì •
```

---

### 2. requirements.txt

```txt
livekit==0.10.0
livekit-agents==0.8.0
openai==1.12.0
anthropic==0.18.0
google-cloud-speech==2.20.0
python-dotenv==1.0.0
```

---

### 3. main.py

```python
import asyncio
import logging
from livekit import rtc
from livekit.agents import AutoSubscribe, JobContext, WorkerOptions, cli, llm
from agent import EndoscopyConsultationAgent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def entrypoint(ctx: JobContext):
    """Agent ì§„ì…ì """
    logger.info(f"Starting agent for room: {ctx.room.name}")

    # Agent ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    agent = EndoscopyConsultationAgent(ctx)

    # Room ì—°ê²°
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)

    # Agent ì‹œì‘
    await agent.start()

    logger.info("Agent started successfully")


if __name__ == "__main__":
    # Worker ì‹¤í–‰
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
```

---

### 4. agent.py

```python
import asyncio
import json
from typing import Optional
from livekit import rtc
from livekit.agents import JobContext, llm, stt, tts, metrics
import openai


class EndoscopyConsultationAgent:
    """ë‚´ì‹œê²½ ê²€ì‚¬ ìƒë‹´ Agent"""

    def __init__(self, ctx: JobContext):
        self.ctx = ctx
        self.room = ctx.room
        self.participant: Optional[rtc.RemoteParticipant] = None

        # AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.stt_service = stt.STT(provider="openai")  # Whisper
        self.llm_service = llm.LLM(provider="openai", model="gpt-4")
        self.tts_service = tts.TTS(provider="openai", voice="alloy")

        # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
        self.conversation_history = []

        # ë‚´ì‹œê²½ ê²€ì‚¬ ê´€ë ¨ ì§€ì‹ ë² ì´ìŠ¤
        self.knowledge_base = self._load_knowledge_base()

        # RPC í•¸ë“¤ëŸ¬ ë“±ë¡
        self.room.local_participant.register_rpc_method("interrupt", self.handle_interrupt)

    def _load_knowledge_base(self) -> dict:
        """ë‚´ì‹œê²½ ê²€ì‚¬ FAQ ë¡œë“œ"""
        return {
            "questions": [
                {
                    "question": "ìˆ˜ë©´ ë‚´ì‹œê²½ë„ ê³ í†µì„ ëŠë‚„ ìˆ˜ ìˆë‚˜ìš”?",
                    "answer": "ìˆ˜ë©´ ë‚´ì‹œê²½ì€ ì§„ì •ì œë¥¼ ì‚¬ìš©í•˜ì—¬ í™˜ìê°€ í¸ì•ˆí•œ ìƒíƒœì—ì„œ ê²€ì‚¬ë¥¼ ë°›ë„ë¡ í•©ë‹ˆë‹¤. "
                             "ëŒ€ë¶€ë¶„ì˜ í™˜ìëŠ” ê²€ì‚¬ ì¤‘ ë¶ˆí¸í•¨ì„ ê±°ì˜ ëŠë¼ì§€ ëª»í•˜ë©°, ê²€ì‚¬ í›„ì—ë„ í†µì¦ì´ ê²½ë¯¸í•©ë‹ˆë‹¤. "
                             "ë‹¤ë§Œ, ê°œì¸ì°¨ê°€ ìˆì–´ ì¼ë¶€ í™˜ìëŠ” ì•½ê°„ì˜ ë¶ˆí¸í•¨ì„ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                },
                {
                    "question": "ê²€ì‚¬ ì „ ê¸ˆì‹ì€ ì–¼ë§ˆë‚˜ í•´ì•¼ í•˜ë‚˜ìš”?",
                    "answer": "ìœ„ë‚´ì‹œê²½ì˜ ê²½ìš° ê²€ì‚¬ 8ì‹œê°„ ì „ë¶€í„° ê¸ˆì‹ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                             "ë¬¼ì€ ê²€ì‚¬ 2ì‹œê°„ ì „ê¹Œì§€ ì†ŒëŸ‰ ì„­ì·¨ ê°€ëŠ¥í•˜ë©°, ë‹¹ì¼ ì•„ì¹¨ ì•½ì€ ì˜ì‚¬ì™€ ìƒë‹´ í›„ ë³µìš© ì—¬ë¶€ë¥¼ ê²°ì •í•˜ì…”ì•¼ í•©ë‹ˆë‹¤."
                },
                {
                    "question": "ê²€ì‚¬ í›„ ìš´ì „ì´ ê°€ëŠ¥í•œê°€ìš”?",
                    "answer": "ìˆ˜ë©´ ë‚´ì‹œê²½ì˜ ê²½ìš° ì§„ì •ì œ ì˜í–¥ìœ¼ë¡œ ê²€ì‚¬ ë‹¹ì¼ ìš´ì „ì€ ê¸ˆì§€ë©ë‹ˆë‹¤. "
                             "ë³´í˜¸ì ë™ë°˜ì´ í•„ìš”í•˜ë©°, ëŒ€ì¤‘êµí†µì´ë‚˜ íƒì‹œ ì´ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤. "
                             "ì¼ë°˜ ë‚´ì‹œê²½ì˜ ê²½ìš° ìš´ì „ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
                }
            ]
        }

    async def start(self):
        """Agent ì‹œì‘"""
        # ì°¸ì—¬ì ëŒ€ê¸°
        self.participant = await self._wait_for_participant()

        if not self.participant:
            logger.error("No participant found")
            return

        logger.info(f"Patient connected: {self.participant.identity}")

        # í™˜ì˜ ë©”ì‹œì§€
        await self._speak("ì•ˆë…•í•˜ì„¸ìš”. AI ì˜ì‚¬ì…ë‹ˆë‹¤. ë‚´ì‹œê²½ ê²€ì‚¬ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ì ì„ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”.")

        # ì˜¤ë””ì˜¤ íŠ¸ë™ êµ¬ë…
        await self._subscribe_to_audio()

    async def _wait_for_participant(self, timeout: int = 30) -> Optional[rtc.RemoteParticipant]:
        """í™˜ì ì°¸ì—¬ ëŒ€ê¸°"""
        start_time = asyncio.get_event_loop().time()

        while asyncio.get_event_loop().time() - start_time < timeout:
            for participant in self.room.remote_participants.values():
                if not participant.identity.startswith("agent"):
                    return participant
            await asyncio.sleep(0.5)

        return None

    async def _subscribe_to_audio(self):
        """í™˜ì ì˜¤ë””ì˜¤ íŠ¸ë™ êµ¬ë…"""
        for publication in self.participant.track_publications.values():
            if publication.kind == rtc.TrackKind.KIND_AUDIO:
                await publication.set_subscribed(True)
                publication.track.on("frame_received", self._on_audio_frame)

    async def _on_audio_frame(self, frame: rtc.AudioFrame):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹  (ìŒì„± ì¸ì‹)"""
        # STT ì²˜ë¦¬
        text = await self.stt_service.recognize(frame.data)

        if text:
            logger.info(f"Patient said: {text}")

            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            self.conversation_history.append({
                "role": "user",
                "content": text
            })

            # LLM ì‘ë‹µ ìƒì„±
            response = await self._generate_response(text)

            # TTS ë³€í™˜ ë° ì „ì†¡
            await self._speak(response)

    async def _generate_response(self, user_input: str) -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """
        ë‹¹ì‹ ì€ ë‚´ì‹œê²½ ê²€ì‚¬ ì „ë¬¸ ì˜ë£Œ ìƒë‹´ AIì…ë‹ˆë‹¤.
        í™˜ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        ì˜í•™ì  ì¡°ì–¸ì€ ì¼ë°˜ì ì¸ ì •ë³´ë§Œ ì œê³µí•˜ê³ , êµ¬ì²´ì ì¸ ì§„ë‹¨ì´ë‚˜ ì¹˜ë£ŒëŠ” ì˜ì‚¬ì™€ ìƒë‹´í•˜ë„ë¡ ì•ˆë‚´í•˜ì„¸ìš”.
        """

        # FAQì—ì„œ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰
        similar_answer = self._search_faq(user_input)
        if similar_answer:
            context = f"[FAQ ì°¸ê³ ]: {similar_answer}"
        else:
            context = ""

        # LLM í˜¸ì¶œ
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "system", "content": context},
            *self.conversation_history
        ]

        response = await self.llm_service.generate(messages)

        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_history.append({
            "role": "assistant",
            "content": response
        })

        return response

    def _search_faq(self, query: str) -> Optional[str]:
        """FAQ ê²€ìƒ‰ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)"""
        query_lower = query.lower()

        for item in self.knowledge_base["questions"]:
            if any(keyword in query_lower for keyword in ["ê³ í†µ", "ì•„í””", "í†µì¦"]):
                return item["answer"]
            elif any(keyword in query_lower for keyword in ["ê¸ˆì‹", "ë¨¹", "ìŒì‹"]):
                return item["answer"]
            elif any(keyword in query_lower for keyword in ["ìš´ì „", "ì°¨"]):
                return item["answer"]

        return None

    async def _speak(self, text: str):
        """TTS ìŒì„± ì „ì†¡"""
        logger.info(f"Agent speaking: {text}")

        # TTS ë³€í™˜
        audio_data = await self.tts_service.synthesize(text)

        # ì˜¤ë””ì˜¤ íŠ¸ë™ìœ¼ë¡œ ì „ì†¡
        audio_source = rtc.AudioSource(sample_rate=24000, num_channels=1)
        track = rtc.LocalAudioTrack.create_audio_track("agent-audio", audio_source)

        # íŠ¸ë™ ë°œí–‰
        await self.room.local_participant.publish_track(track)

        # ì˜¤ë””ì˜¤ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
        await audio_source.capture_frame(audio_data)

        # ë¦½ì‹±í¬ ë°ì´í„° ì „ì†¡ (ë¸”ë Œë“œì…°ì´í”„)
        lip_sync_data = self._generate_lip_sync(audio_data)
        await self._send_lip_sync_data(lip_sync_data)

    def _generate_lip_sync(self, audio_data: bytes) -> list:
        """ë¦½ì‹±í¬ ë¸”ë Œë“œì…°ì´í”„ ìƒì„±"""
        # ì‹¤ì œë¡œëŠ” Fluentt TalkMotion API ë˜ëŠ” Oculus LipSync ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜

        # ì˜¤ë””ì˜¤ ë³¼ë¥¨ ê¸°ë°˜ ê°„ë‹¨í•œ ë¦½ì‹±í¬
        volume = sum(abs(sample) for sample in audio_data) / len(audio_data)

        return [
            volume * 0.8,  # A
            volume * 0.5,  # I
            volume * 0.3,  # U
            volume * 0.4,  # E
            volume * 0.6,  # O
            0.0            # Blink
        ]

    async def _send_lip_sync_data(self, blend_shapes: list):
        """ë¦½ì‹±í¬ ë°ì´í„°ë¥¼ Unityë¡œ ì „ì†¡"""
        data = json.dumps({
            "type": "lipSync",
            "blendShapes": blend_shapes
        }).encode('utf-8')

        # Data Channelë¡œ ì „ì†¡
        await self.room.local_participant.publish_data(
            data,
            kind=rtc.DataPacketKind.KIND_RELIABLE
        )

    async def handle_interrupt(self, data: rtc.RpcInvocationData) -> str:
        """RPC: ë‹µë³€ ì¤‘ë‹¨"""
        logger.info("Interrupt signal received from patient")

        # TTS ì¤‘ë‹¨
        # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” TTS ìŠ¤íŠ¸ë¦¬ë° ì¤‘ë‹¨ ë¡œì§ í•„ìš”)

        return json.dumps({"success": True})
```

---

### 5. config.py

```python
import os
from dotenv import load_dotenv

load_dotenv()


class Config:
    # LiveKit
    LIVEKIT_URL = os.getenv("LIVEKIT_URL", "ws://localhost:7880")
    LIVEKIT_API_KEY = os.getenv("LIVEKIT_API_KEY")
    LIVEKIT_API_SECRET = os.getenv("LIVEKIT_API_SECRET")

    # OpenAI
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # Anthropic
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

    # Agent Settings
    AGENT_NAME = "EndoscopyConsultationAgent"
    AGENT_VERSION = "1.0.0"
```

---

## ğŸš€ ë°°í¬ ë° ì‹¤í–‰

### 1. Unity WebGL ë¹Œë“œ

```bash
# Unity Editorì—ì„œ ë¹Œë“œ
File â†’ Build Settings â†’ WebGL â†’ Build

# ë¹Œë“œ ê²°ê³¼ë¬¼
unity-webgl/Build/
â”œâ”€â”€ Build.data
â”œâ”€â”€ Build.framework.js
â”œâ”€â”€ Build.loader.js
â””â”€â”€ Build.wasm

# React public í´ë”ë¡œ ë³µì‚¬
cp -r unity-webgl/Build/* web/public/unity/
```

---

### 2. Python Agent ì‹¤í–‰

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
cd agent
pip install -r requirements.txt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export LIVEKIT_URL=wss://livekit.mirabel.com
export LIVEKIT_API_KEY=your_api_key
export LIVEKIT_API_SECRET=your_api_secret
export OPENAI_API_KEY=your_openai_key

# Agent ì‹¤í–‰
python main.py start
```

---

### 3. ì „ì²´ ì‹œìŠ¤í…œ Docker Compose

```yaml
version: '3.8'

services:
  # LiveKit Server
  livekit:
    image: livekit/livekit-server:latest
    ports:
      - "7880:7880"   # WebSocket
      - "7881:7881"   # HTTP
    environment:
      - LIVEKIT_KEYS=${LIVEKIT_API_KEY}:${LIVEKIT_API_SECRET}
    volumes:
      - ./livekit.yaml:/etc/livekit.yaml

  # Python Agent
  agent:
    build: ./agent
    environment:
      - LIVEKIT_URL=ws://livekit:7880
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - livekit

  # React Web App
  web:
    build: ./web
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_LIVEKIT_URL=wss://livekit.mirabel.com
    depends_on:
      - livekit
```

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### 1. Unity ì½˜ì†” ë¡œê·¸

```csharp
// Debug.Logë¥¼ ë¸Œë¼ìš°ì € ì½˜ì†”ë¡œ ì¶œë ¥
Debug.Log("LiveKit connected");
// â†’ ë¸Œë¼ìš°ì € Consoleì—ì„œ í™•ì¸ ê°€ëŠ¥
```

### 2. LiveKit Inspector

```
http://localhost:7881/inspector
```

LiveKit Inspectorì—ì„œ ë‹¤ìŒì„ í™•ì¸ ê°€ëŠ¥:
- Room ìƒíƒœ
- Participant ëª©ë¡
- Track ì •ë³´
- ë„¤íŠ¸ì›Œí¬ í’ˆì§ˆ

### 3. Performance Profiler

```typescript
// Reactì—ì„œ ì„±ëŠ¥ ì¸¡ì •
import { performance } from 'perf_hooks';

const startTime = performance.now();
// ... Unity ë¡œë”©
const loadTime = performance.now() - startTime;
console.log(`Unity loaded in ${loadTime}ms`);
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Unity WebGL ë¡œë”© ì‹¤íŒ¨

**ì¦ìƒ**: Unityê°€ ë¡œë”©ë˜ì§€ ì•ŠìŒ

**í•´ê²°ì±…**:
```typescript
// ë¸Œë¼ìš°ì € í˜¸í™˜ì„± í™•ì¸
const isWebGLSupported = (() => {
  try {
    const canvas = document.createElement('canvas');
    return !!(canvas.getContext('webgl') || canvas.getContext('experimental-webgl'));
  } catch (e) {
    return false;
  }
})();

if (!isWebGLSupported) {
  alert('ì´ ë¸Œë¼ìš°ì €ëŠ” WebGLì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
}
```

---

### 2. ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€

**ì¦ìƒ**: ìŒì„± ì…ë ¥ì´ ì•ˆ ë¨

**í•´ê²°ì±…**:
```typescript
async function requestMicrophonePermission() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    stream.getTracks().forEach(track => track.stop());
    return true;
  } catch (error) {
    alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ë¥¼ í—ˆìš©í•´ì£¼ì„¸ìš”.');
    return false;
  }
}
```

---

### 3. ë¦½ì‹±í¬ ì§€ì—°

**ì¦ìƒ**: ì•„ë°”íƒ€ ì… ëª¨ì–‘ê³¼ ì˜¤ë””ì˜¤ ì‹±í¬ ì•ˆ ë§ìŒ

**í•´ê²°ì±…**:
```csharp
// Unityì—ì„œ ì˜¤ë””ì˜¤ ì§€ì—° ë³´ìƒ
[SerializeField] private float lipSyncDelay = 0.1f; // 100ms ì§€ì—°

private IEnumerator ApplyLipSyncWithDelay(float[] blendShapes)
{
    yield return new WaitForSeconds(lipSyncDelay);
    avatarController.ApplyLipSync(blendShapes);
}
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **LiveKit Documentation**: https://docs.livekit.io/
- **LiveKit Unity SDK**: https://github.com/livekit/client-sdk-unity
- **UniVRM**: https://github.com/vrm-c/UniVRM
- **Oculus LipSync**: https://developer.oculus.com/downloads/package/oculus-lipsync-unity/

---

## ğŸ“ ë¬¸ì˜

LiveKit í†µí•© ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì€ ë‹¤ìŒìœ¼ë¡œ ì—°ë½ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤:
- **Unity Developer**: unity@example.com
- **AI Engineer**: ai@example.com
- **Slack**: #mirabel-livekit
